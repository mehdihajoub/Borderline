{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22155,"status":"ok","timestamp":1716566161376,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"Oqbqf2gysFBM","outputId":"a978620c-5893-4d44-acd6-234c83c74fd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":293,"status":"ok","timestamp":1716566161377,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"doLyuqXksO3P"},"outputs":[],"source":["!ln -s \"/content/drive/My Drive/Borderline/\" \"/content/\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1716566161377,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"p5N1nPWbsPYh","outputId":"e5a301f6-7154-4a56-b831-f434216256d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/Borderline\n"]}],"source":["%cd Borderline"]},{"cell_type":"markdown","metadata":{"id":"VX8PQvzZszCO"},"source":["Installs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CwZfdRQEscId"},"outputs":[],"source":["%%capture\n","import torch\n","major_version, minor_version = torch.cuda.get_device_capability()\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","if major_version \u003e= 8:\n","    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n","else:\n","    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hSLC1E9hs8D8"},"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import Adam\n","from torch.utils.data import Subset, DataLoader, random_split, Dataset\n","from torch.optim import lr_scheduler\n","import torch.utils.data as data\n","from torchvision import transforms\n","from transformers import BertTokenizer, AdamW, BertForSequenceClassification\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","import moviepy.audio as audio\n","import moviepy.editor as mp\n","\n","from unsloth import FastLanguageModel\n","\n","import json\n","from transformers import AutoTokenizer\n","from datasets import Dataset\n","import numpy as np\n","import os\n","import time\n","import copy\n","import random\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"8z9-mToxvNwG"},"source":["Loading quantized versions of Llama 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwPyWww-s5wn"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! Llama 3 is up to 8k\n","dtype = None\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","fourbit_models = [\n","    \"unsloth/mistral-7b-bnb-4bit\",\n","    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n","    \"unsloth/llama-2-7b-bnb-4bit\",\n","    \"unsloth/gemma-7b-bnb-4bit\",\n","    \"unsloth/gemma-7b-it-bnb-4bit\",\n","    \"unsloth/gemma-2b-bnb-4bit\",\n","    \"unsloth/gemma-2b-it-bnb-4bit\",\n","    \"unsloth/llama-3-8b-bnb-4bit\",\n","]\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/llama-3-8b-bnb-4bit\", # Llama-3 70b also works (just change the model name)\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MV28LXTH_w0"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number \u003e 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1RurQ1I_WsK"},"outputs":[],"source":["import json\n","from datasets import Dataset\n","import pyarrow as pa\n","from typing import Optional\n","\n","class OppoDataset:\n","    def __init__(self, file_path: str):\n","        self.file_path = file_path\n","\n","    def _load_data(self):\n","        with open(self.file_path, 'r') as file:\n","            data = json.load(file)\n","        return data\n","\n","    def _format_data(self, data):\n","        formatted_data = {\n","            \"instruction\": [],\n","            \"input\": [],\n","            \"output\": []\n","        }\n","        for entry in data:\n","            formatted_data[\"instruction\"].append(entry[\"instruction\"])\n","            formatted_data[\"input\"].append(entry[\"input\"])\n","            formatted_data[\"output\"].append(entry[\"output\"])\n","        return formatted_data\n","\n","    def to_dataset(self):\n","        data = self._load_data()\n","        formatted_data = self._format_data(data)\n","        arrow_table = pa.Table.from_pydict(formatted_data)\n","        return Dataset(arrow_table)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mST51fs049UB"},"outputs":[],"source":["# this is basically the system prompt\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token # do not forget this part!\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN # without this token generation goes on forever!\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","\n","# Instantiate the helper class\n","datapath = 'data/oppodata.json'\n","dataset_helper = OppoDataset(datapath)\n","\n","# Convert to Hugging Face dataset\n","dataset = dataset_helper.to_dataset()\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"markdown","metadata":{"id":"n3TeQGB9Ccml"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vmr8B5uC52Gj"},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        max_steps = 60,\n","        num_train_epochs=4,\n","        learning_rate = 2e-4,\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        bf16 = torch.cuda.is_bf16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yv57FS7-IPQt"},"outputs":[],"source":["trainer_stats = trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNEECzwDnv06kJuUyfQ049c","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}