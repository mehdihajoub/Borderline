{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20872,"status":"ok","timestamp":1716565953395,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"9q-GfZjNpjGc","outputId":"c0c1eeec-d51e-4f3c-8a3a-b6995c950d9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UGFHpEdhpntw","executionInfo":{"status":"ok","timestamp":1716565956332,"user_tz":-120,"elapsed":416,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"}}},"outputs":[],"source":["!ln -s \"/content/drive/My Drive/Borderline/\" \"/content/\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83,"status":"ok","timestamp":1716565958446,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"Y0qr-yzcpp9l","outputId":"68e8031f-852b-457d-ef54-653eb3b98358"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Borderline\n"]}],"source":["%cd Borderline"]},{"cell_type":"markdown","metadata":{"id":"gJLtbT-WzREN"},"source":["# Classification"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6115,"status":"ok","timestamp":1716552187055,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"jxxbLvr_6f71","outputId":"1394e572-b55c-44c5-cbf8-d4696c985883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.4)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n"]}],"source":["pip install SpeechRecognition"]},{"cell_type":"code","source":["%%capture\n","# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"],"metadata":{"id":"NdP3XINyOZzZ","executionInfo":{"status":"ok","timestamp":1716552243399,"user_tz":-120,"elapsed":41068,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qPmK_2c-p9k4"},"source":["## Imports"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IeWUNhEyp9cF","executionInfo":{"status":"ok","timestamp":1716552259807,"user_tz":-120,"elapsed":2249,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import Adam\n","from torch.utils.data import Subset, DataLoader, random_split, Dataset\n","from torch.optim import lr_scheduler\n","import torch.utils.data as data\n","from torchvision import transforms\n","from transformers import BertTokenizer, AdamW, BertForSequenceClassification\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","import speech_recognition as sr\n","import moviepy.audio as audio\n","import moviepy.editor as mp\n","\n","from unsloth import FastLanguageModel\n","\n","import json\n","from transformers import AutoTokenizer\n","from datasets import Dataset\n","import numpy as np\n","import os\n","import time\n","import copy\n","import random\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1271,"status":"ok","timestamp":1716466141936,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"qD9TCGqQCA96","outputId":"ebf442a3-08bb-491b-b178-b5eef31de5ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has been successfully converted and saved to final_dataset.json\n"]}],"source":["import json\n","\n","def convert_to_alpaca_format(input_filename, output_filename):\n","    # Load the original JSON data from the file\n","    with open(input_filename, 'r') as file:\n","        data = json.load(file)\n","\n","    # Prepare data in the new Alpaca prompt format\n","    alpaca_data = []\n","    for entry in data:\n","        alpaca_prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Turn the input phrase into its opposite phrase\n","\n","### Input:\n","{entry['Sentence']}\n","\n","### Response:\n","{entry['Opposite']}\n","\"\"\"\n","        alpaca_data.append(alpaca_prompt)\n","\n","    # Save the transformed data to a new JSON file\n","    with open(output_filename, 'w') as outfile:\n","        json.dump(alpaca_data, outfile, indent=4)\n","\n","# Specify the input and output file names\n","input_filename = 'data/dataset.json'\n","output_filename = 'final_dataset.json'\n","\n","# Run the conversion function\n","convert_to_alpaca_format(input_filename, output_filename)\n","\n","print(f\"Data has been successfully converted and saved to {output_filename}\")"]},{"cell_type":"markdown","metadata":{"id":"DEQ4QQ4gqFDY"},"source":["#Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hk3bg1aNp8uO"},"outputs":[],"source":["import pandas as pd\n","from torch.utils.data import Dataset\n","from transformers import BertTokenizer\n","\n","class TweetsDataset(Dataset):\n","    def __init__(self, filename, tokenizer, max_len):\n","        self.data = pd.read_csv(filename)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.LABEL_MAP = {'hate': 1, 'nothate': 0}\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        tweet = self.data.loc[index, 'text']\n","        label = self.LABEL_MAP[self.data.loc[index, 'label']]\n","\n","        encodings = self.tokenizer(tweet, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n","        return {\n","            'input_ids': encodings['input_ids'].squeeze(),  # Use squeeze to remove batch dimension\n","            'attention_mask': encodings['attention_mask'].squeeze(),\n","            'labels': torch.tensor(label, dtype=torch.long)  # Use torch.long for labels\n","        }"]},{"cell_type":"markdown","metadata":{"id":"MfGm-EtVqRHN"},"source":["#Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLv8gbIsqQd1"},"outputs":[],"source":["MAX_LEN = 128\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-5\n","NUM_EPOCHS = 10\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"qox95IEer7a8"},"source":["# Dataset init and Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y66NoxxvqL5d"},"outputs":[],"source":["csv_path = 'data/data.csv'\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","dataset = TweetsDataset(csv_path, tokenizer, MAX_LEN)\n","\n","dataset_size = int(len(dataset)/8)\n","dataset_indices = list(range(dataset_size))\n","train_indices, test_indices = train_test_split(dataset_indices, test_size=0.2, random_state=42)\n","\n","train_subset = Subset(dataset, train_indices)\n","test_subset = Subset(dataset, test_indices)\n","\n","train_loader = DataLoader(train_subset, BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_subset, BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"IzTNC9uFsICi"},"source":["#Pre-trained Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1371,"status":"ok","timestamp":1716379979399,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"m5ArsiOwsHcQ","outputId":"6202e191-2a16-4cd7-e52e-324896ea1bac"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":53}],"source":["classifier = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n","classifier.to(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"h8Fdilj8sZXG"},"source":["# Optimizier and Learning Rate Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1716379982697,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"VqBQpBehscPI","outputId":"816a93ba-1801-4529-f40b-299ee85824b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n"]}],"source":["optimizer = AdamW(classifier.parameters(), lr=LEARNING_RATE)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":8826,"status":"error","timestamp":1716379993404,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"JX7wZ-KfsdSU","outputId":"05cb9b5b-3102-4c42-a210-850e202ec64e"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-76d0b7029f41>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["classifier.train()\n","\n","for epoch in range(NUM_EPOCHS):\n","    epoch_loss = 0\n","    for idx, batch in enumerate(train_loader):\n","        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n","        outputs = classifier(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        scheduler.step()\n","        epoch_loss += loss.item()\n","\n","    epoch_loss = epoch_loss/BATCH_SIZE\n","    print(f'Epoch {epoch}', 'Epoch loss :', epoch_loss )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7696,"status":"ok","timestamp":1715534937012,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"OFE6EIeovbQt","outputId":"d5b48abd-7493-4889-b399-29c24743fe94"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.79      0.82      0.80       494\n","           1       0.82      0.80      0.81       535\n","\n","    accuracy                           0.81      1029\n","   macro avg       0.81      0.81      0.81      1029\n","weighted avg       0.81      0.81      0.81      1029\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","classifier.eval()\n","predictions, true_labels = [], []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n","        outputs = classifier(**batch)\n","        logits = outputs.logits\n","        preds = torch.argmax(logits, dim=1)\n","        predictions.extend(preds.cpu().numpy())\n","        true_labels.extend(batch['labels'].cpu().numpy())\n","\n","print(classification_report(true_labels, predictions))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZy4Kqv-wNik"},"outputs":[],"source":["torch.save(classifier.state_dict(), 'classifier_v4.pth')"]},{"cell_type":"markdown","metadata":{"id":"BxnPQHL2oXfp"},"source":["#Testing Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"elapsed":90988,"status":"error","timestamp":1716380105361,"user":{"displayName":"Mehdi Hajoub","userId":"10367422732796270715"},"user_tz":-120},"id":"V-1cHMzkm7Qn","outputId":"3ce16f4e-7dac-45fa-e7ab-7be9044855bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Hate Speech Classification Model\n","Type 'quit' to exit.\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-fd59b453a3cc>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a phrase to classify: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n","model.load_state_dict(torch.load('classifier_v4.pth', map_location=device))\n","model.to(device)\n","model.eval()\n","\n","def predict_hate_speech(text):\n","    # Preprocess text\n","    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n","    input_ids = inputs['input_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","\n","    # Predict\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]  # Get the index of the max logit\n","        return \"Hate Speech\" if prediction == 1 else \"Not Hate Speech\"\n","\n","# Running the prediction loop\n","print(\"Hate Speech Classification Model\")\n","print(\"Type 'quit' to exit.\\n\")\n","\n","while True:\n","    user_input = input(\"Enter a phrase to classify: \")\n","    if user_input.lower() == 'quit':\n","        break\n","\n","    prediction = predict_hate_speech(user_input)\n","    print(f\"The phrase is classified as: {prediction}\\n\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNDNj+4ck2BSx+8y4PtVgGu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}